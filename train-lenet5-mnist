import torch

def train_model(model, dataset, loss_fct, optimizer, num_epochs):

    # Determine device we are running on
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # To monitor loss
    loss_history = []
    
    # Traning loop
    num_batches = len(dataset)
    try:
        for epoch in range(num_epochs):
            for i, (images, labels) in enumerate(dataset):

                # Run model on current batch and evaluate loss
                images  = images.to(device)
                labels  = labels.to(device)
                outputs = model(images)
                loss    = loss_fct(outputs, labels)
        	
                # Evaluate gradient and perform SGD step
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # Log loss
                loss_history.append(loss.item())
        		
                if (i+1) % 100 == 0:
                    print(f"Epoch [{epoch+1}/{num_epochs}], "
                          f"batch [{i+1}/{num_batches}], Loss: {loss.item():.4f}")
                    
                if (i+1) % 1000 == 0:
                    import json
                    with open("loss.json", "w") as ofile:
                        json.dump(loss_history, ofile)
            
    finally:
        torch.save(model.state_dict(), "trained-weights.pt")

if __name__ == "__main__":

    torch.manual_seed(0)

    from data import get_datasets
    from model import LeNet5
    from torch import nn
    
    # Instantiate model, dataset, loss function, optimizer
    model = LeNet5()
    loss_fct = nn.CrossEntropyLoss()
    train_set, test_set = get_datasets(batch_size=64)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    # Run training loop
    train_model(model, train_set, loss_fct, optimizer, num_epochs=100)
    
    # 1. Create a multi-batch input (x = torch.rand([4, 3, 224, 224])
    # 2. Set your input to be differentiable (x.requires_grad = True)
    # 3. Run a forward pass (out = model(x))
    # 4. Define the loss as depending from only one of the inputs (for instance: loss = out[2].sum())
    # 5. Run a backprop (loss.backward)
    # 6. Verify that only x[2] has non-null gradients: assert (x.grad[i] == 0.).all() for i != 2 and (x.grad[2] != 0).any()
    
    # Note: you will want to set your model into evaluation mode (model.eval()),
    # otherwise batch norm ops will make this test fail
